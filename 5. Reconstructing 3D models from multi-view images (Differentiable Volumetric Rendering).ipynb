{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ebf301",
   "metadata": {},
   "source": [
    "## 1. Understanding  Ray Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820750b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries and Packages\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras,\n",
    "    PointLights,\n",
    "    look_at_view_transform,\n",
    "    NDCGridRaysampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4146747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CPU only, this will be more slowly\n"
     ]
    }
   ],
   "source": [
    "# Setting up PyTorch device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: CPU only, this will be more slowly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abab0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a batch of 10 cameras \n",
    "num_views: int = 10\n",
    "azimuth_range: float = 180\n",
    "elev = torch.linspace(0, 0, num_views)  # keep constant\n",
    "azim = torch.linspace(-azimuth_range, azimuth_range, num_views) + 180.0\n",
    "lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3194790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a ray sampler\n",
    "image_size = 64\n",
    "volume_extent_world = 3.0\n",
    "\n",
    "raysampler = NDCGridRaysampler(\n",
    "    image_width=image_size,\n",
    "    image_height=image_size,\n",
    "    n_pts_per_ray=50,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c374fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letting the ray sampler know where our cameras are and in what directions they are pointing\n",
    "# Getting the sampled rays and points into ray_bundle\n",
    "ray_bundle = raysampler(cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81991672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray_bundle origins tensor shape =  torch.Size([10, 64, 64, 3])\n",
      "ray_bundle directions shape =  torch.Size([10, 64, 64, 3])\n",
      "ray_bundle lengths =  torch.Size([10, 64, 64, 50])\n",
      "ray_bundle xys shape =  torch.Size([10, 64, 64, 2])\n"
     ]
    }
   ],
   "source": [
    "# Printing out information on ray bundle\n",
    "print('ray_bundle origins tensor shape = ', ray_bundle.origins.shape)\n",
    "print('ray_bundle directions shape = ', ray_bundle.directions.shape)\n",
    "print('ray_bundle lengths = ', ray_bundle.lengths.shape)\n",
    "print('ray_bundle xys shape = ', ray_bundle.xys.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5383a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving ray bundle\n",
    "torch.save({\n",
    "    'ray_bundle': ray_bundle\n",
    "}, 'output/ray_sampling.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9d3a9",
   "metadata": {},
   "source": [
    "## 2. Using Volume Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb6db0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries and Packages\n",
    "import torch\n",
    "from pytorch3d.structures import Volumes\n",
    "from pytorch3d.renderer.implicit.renderer import VolumeSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3be9cd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CPU only, this will be more slowly\n"
     ]
    }
   ],
   "source": [
    "# Setting up PyTorch device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: CPU only, this will be more slowly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab4414d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading ray_bundle computed in the first step\n",
    "checkpoint = torch.load('output/ray_sampling.pt')\n",
    "ray_bundle = checkpoint.get('ray_bundle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5bfb25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining volume\n",
    "batch_size = 10\n",
    "densities = torch.zeros([batch_size, 1, 64, 64, 64]).to(device)\n",
    "colors = torch.zeros(batch_size, 3, 64, 64, 64).to(device)\n",
    "voxel_size = 0.1\n",
    "\n",
    "volumes = Volumes(\n",
    "    densities=densities,\n",
    "    features=colors,\n",
    "    voxel_size=voxel_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0d85478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining volume_sampler\n",
    "volume_sampler = VolumeSampler(volumes = volumes, sample_mode = \"bilinear\")\n",
    "rays_densities, rays_features = volume_sampler(ray_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd73f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rays_densities shape =  torch.Size([10, 64, 64, 50, 1])\n",
      "rays_features shape =  torch.Size([10, 64, 64, 50, 3])\n"
     ]
    }
   ],
   "source": [
    "# Printing out the shapes for the densities and colors\n",
    "print('rays_densities shape = ', rays_densities.shape)\n",
    "print('rays_features shape = ', rays_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b57d1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the densities and colors (to be able to use them in the next step)\n",
    "torch.save({\n",
    "    'rays_densities': rays_densities,\n",
    "    'rays_features': rays_features\n",
    "}, 'output/volume_sampling.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e8507",
   "metadata": {},
   "source": [
    "## 3. Exploring the Ray Marcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f9c85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries and Packages\n",
    "import torch\n",
    "from pytorch3d.renderer.implicit.raymarching import EmissionAbsorptionRaymarcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14f2c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading densities and colors on rays from the second step\n",
    "checkpoint = torch.load('output/volume_sampling.pt')\n",
    "rays_densities = checkpoint.get('rays_densities')\n",
    "rays_features = checkpoint.get('rays_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6354585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining ray_marcher and passing the densities and colors \n",
    "# it will give us the rendered RGB values (image_features)\n",
    "ray_marcher = EmissionAbsorptionRaymarcher()\n",
    "\n",
    "image_features = ray_marcher(rays_densities = rays_densities, \n",
    "                             rays_features = rays_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e754793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_features shape =  torch.Size([10, 64, 64, 4])\n"
     ]
    }
   ],
   "source": [
    "# Printing out the image feature shape\n",
    "print('image_features shape = ', image_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706b0a7",
   "metadata": {},
   "source": [
    "So, we have a batch of 10 images 64x64, and 4 channels: the first three - RGBs; the last one - the alpha channel (it represents whether the pixel is in the foreground or the background)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf1d94",
   "metadata": {},
   "source": [
    "## 4. Reconstructing 3D Models from Multi-view Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2fd5bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries and Packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from pytorch3d.structures import Volumes\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras,\n",
    "    VolumeRenderer,\n",
    "    NDCGridRaysampler,\n",
    "    EmissionAbsorptionRaymarcher\n",
    ")\n",
    "from pytorch3d.transforms import so3_exp_map\n",
    "\n",
    "\n",
    "# Specific files downloaded from \n",
    "# https://github.com/PacktPublishing/3D-Deep-Learning-with-Python/tree/main/chap5\n",
    "from plot_image_grid import image_grid\n",
    "from generate_cow_renders import generate_cow_renders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8889b167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CPU only, this will be more slowly\n"
     ]
    }
   ],
   "source": [
    "# Setting up PyTorch device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: CPU only, this will be more slowly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a53c23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating 40 cameras, images, and silhouette images with different angles\n",
    "target_cameras, target_images, target_silhouettes = generate_cow_renders(num_views=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64fe57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a ray sampler\n",
    "render_size = 128\n",
    "volume_extent_world = 3.0\n",
    "\n",
    "raysampler = NDCGridRaysampler(\n",
    "    image_width=render_size,\n",
    "    image_height=render_size,\n",
    "    n_pts_per_ray=150,\n",
    "    min_depth=0.1,\n",
    "    max_depth=volume_extent_world,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9107607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the ray marcher\n",
    "raymarcher = EmissionAbsorptionRaymarcher()\n",
    "renderer = VolumeRenderer(raysampler=raysampler, raymarcher=raymarcher,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6259db79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class for encapsulating a volume\n",
    "class VolumeModel(torch.nn.Module):\n",
    "    def __init__(self, renderer, volume_size=[64]*3, voxel_size=0.1):\n",
    "        super().__init__()\n",
    "        self.log_densities = torch.nn.Parameter(-4.0 * torch.ones(1, *volume_size))\n",
    "        self.log_colors = torch.nn.Parameter(torch.zeros(3, *volume_size))\n",
    "        self._voxel_size = voxel_size\n",
    "        self._renderer = renderer\n",
    "\n",
    "    def forward(self, cameras):\n",
    "        batch_size = cameras.R.shape[0]\n",
    "\n",
    "        densities = torch.sigmoid(self.log_densities)\n",
    "        colors = torch.sigmoid(self.log_colors)\n",
    "\n",
    "        volumes = Volumes(\n",
    "            densities=densities[None].expand(batch_size, *self.log_densities.shape),\n",
    "            features=colors[None].expand(batch_size, *self.log_colors.shape),\n",
    "            voxel_size=self._voxel_size,\n",
    "        )\n",
    "        return self._renderer(cameras=cameras, volumes=volumes)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88e922f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Huber loss function\n",
    "def huber(x, y, scaling=0.1):\n",
    "    diff_sq = (x - y) ** 2\n",
    "    loss = ((1 + diff_sq / (scaling ** 2)).clamp(1e-4).sqrt() - 1) * float(scaling)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5c05213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving to the right device\n",
    "target_cameras = target_cameras.to(device)\n",
    "target_images = target_images.to(device)\n",
    "target_silhouettes = target_silhouettes.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1635c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating inastance of VolumeModel\n",
    "volume_size = 128\n",
    "\n",
    "volume_model = VolumeModel(\n",
    "    renderer,\n",
    "    volume_size=[volume_size] * 3,\n",
    "    voxel_size=volume_extent_world / volume_size,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65a70286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the optimizer\n",
    "lr = 0.1\n",
    "optimizer = torch.optim.Adam(volume_model.parameters(), lr=lr)\n",
    "\n",
    "batch_size = 10\n",
    "n_iter = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8aa49f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 00000: color_err = 2.67e-01 mask_err = 5.67e-01\n",
      "Iteration 00010: color_err = 1.08e-01 mask_err = 3.57e-01\n",
      "Iteration 00020: color_err = 3.20e-02 mask_err = 1.61e-01\n",
      "Iteration 00030: color_err = 1.34e-02 mask_err = 7.79e-02\n",
      "Iteration 00040: color_err = 9.94e-03 mask_err = 4.45e-02\n",
      "Iteration 00050: color_err = 7.37e-03 mask_err = 3.09e-02\n",
      "Iteration 00060: color_err = 6.26e-03 mask_err = 2.39e-02\n",
      "Iteration 00070: color_err = 5.54e-03 mask_err = 1.89e-02\n",
      "Iteration 00080: color_err = 5.89e-03 mask_err = 1.77e-02\n",
      "Iteration 00090: color_err = 5.13e-03 mask_err = 1.57e-02\n",
      "Iteration 00100: color_err = 4.85e-03 mask_err = 1.48e-02\n",
      "Iteration 00110: color_err = 5.01e-03 mask_err = 1.43e-02\n",
      "Iteration 00120: color_err = 4.88e-03 mask_err = 1.34e-02\n",
      "Iteration 00130: color_err = 4.66e-03 mask_err = 1.25e-02\n",
      "Iteration 00140: color_err = 4.77e-03 mask_err = 1.39e-02\n",
      "Iteration 00150: color_err = 4.37e-03 mask_err = 1.11e-02\n",
      "Iteration 00160: color_err = 4.50e-03 mask_err = 1.14e-02\n",
      "Iteration 00170: color_err = 4.29e-03 mask_err = 1.13e-02\n",
      "Iteration 00180: color_err = 4.53e-03 mask_err = 1.09e-02\n",
      "Iteration 00190: color_err = 3.74e-03 mask_err = 1.05e-02\n",
      "Iteration 00200: color_err = 4.05e-03 mask_err = 1.05e-02\n",
      "Iteration 00210: color_err = 3.88e-03 mask_err = 1.13e-02\n",
      "Iteration 00220: color_err = 3.90e-03 mask_err = 1.08e-02\n",
      "Decreasing LR 10-fold ...\n",
      "Iteration 00230: color_err = 4.01e-03 mask_err = 9.73e-03\n",
      "Iteration 00240: color_err = 4.21e-03 mask_err = 9.45e-03\n",
      "Iteration 00250: color_err = 3.87e-03 mask_err = 1.06e-02\n",
      "Iteration 00260: color_err = 3.90e-03 mask_err = 1.07e-02\n",
      "Iteration 00270: color_err = 3.76e-03 mask_err = 9.82e-03\n",
      "Iteration 00280: color_err = 4.18e-03 mask_err = 1.06e-02\n",
      "Iteration 00290: color_err = 3.92e-03 mask_err = 8.89e-03\n"
     ]
    }
   ],
   "source": [
    "# Running optimization iterrations\n",
    "for iteration in range(n_iter):\n",
    "\n",
    "    if iteration == round(n_iter * 0.75):\n",
    "        print('Decreasing LR 10-fold ...')\n",
    "        optimizer = torch.optim.Adam(\n",
    "            volume_model.parameters(), lr=lr * 0.1\n",
    "        )\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
    "\n",
    "    # Sample the minibatch of cameras\n",
    "    batch_cameras = FoVPerspectiveCameras(\n",
    "        R=target_cameras.R[batch_idx],\n",
    "        T=target_cameras.T[batch_idx],\n",
    "        znear=target_cameras.znear[batch_idx],\n",
    "        zfar=target_cameras.zfar[batch_idx],\n",
    "        aspect_ratio=target_cameras.aspect_ratio[batch_idx],\n",
    "        fov=target_cameras.fov[batch_idx],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    rendered_images, rendered_silhouettes = volume_model(\n",
    "        batch_cameras\n",
    "    ).split([3, 1], dim=-1)\n",
    "\n",
    "    sil_err = huber(\n",
    "        rendered_silhouettes[..., 0], target_silhouettes[batch_idx],\n",
    "    ).abs().mean()\n",
    "\n",
    "    color_err = huber(\n",
    "        rendered_images, target_images[batch_idx],\n",
    "    ).abs().mean()\n",
    "\n",
    "    loss = color_err + sil_err\n",
    "    \n",
    "    if iteration % 10 == 0:\n",
    "        print(\n",
    "            f'Iteration {iteration:05d}:'\n",
    "            + f' color_err = {float(color_err):1.2e}'\n",
    "            + f' mask_err = {float(sil_err):1.2e}'\n",
    "        )\n",
    "\n",
    "    # Take the optimization step.\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de165ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating rotating volume ...\n"
     ]
    }
   ],
   "source": [
    "# Taking the final volumetric model and render images from new angles\n",
    "def generate_rotating_volume(volume_model, n_frames=50):\n",
    "    logRs = torch.zeros(n_frames, 3, device=device)\n",
    "    logRs[:, 1] = torch.linspace(0.0, 2.0 * 3.14, n_frames, device=device)\n",
    "    Rs = so3_exp_map(logRs)\n",
    "    Ts = torch.zeros(n_frames, 3, device=device)\n",
    "    Ts[:, 2] = 2.7\n",
    "    frames = []\n",
    "    print('Generating rotating volume ...')\n",
    "    for R, T in zip(Rs, Ts):\n",
    "        camera = FoVPerspectiveCameras(\n",
    "            R=R[None],\n",
    "            T=T[None],\n",
    "            znear=target_cameras.znear[0],\n",
    "            zfar=target_cameras.zfar[0],\n",
    "            aspect_ratio=target_cameras.aspect_ratio[0],\n",
    "            fov=target_cameras.fov[0],\n",
    "            device=device,\n",
    "        )\n",
    "        frames.append(volume_model(camera)[..., :3].clamp(0.0, 1.0))\n",
    "    return torch.cat(frames)\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    rotating_volume_frames = generate_rotating_volume(volume_model, n_frames=7*4)\n",
    "\n",
    "image_grid(rotating_volume_frames.clamp(0., 1.).cpu().numpy(), \n",
    "           rows=4, cols=7, rgb=True, fill=True)\n",
    "plt.savefig('rotating_volume.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b374d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VolumeModel(\n",
       "  (_renderer): VolumeRenderer(\n",
       "    (renderer): ImplicitRenderer(\n",
       "      (raysampler): NDCMultinomialRaysampler()\n",
       "      (raymarcher): EmissionAbsorptionRaymarcher()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f92f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".3d",
   "language": "python",
   "name": ".3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
