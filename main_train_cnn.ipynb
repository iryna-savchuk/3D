{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT-XuvIMGxZq"
      },
      "source": [
        "## Preparation and Downloads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zclaeZ6xGln3"
      },
      "source": [
        "### Loading Libraries and Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h8FN5IT9e03f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Optimizer\n",
        "\n",
        "from tensorflow.keras.metrics import Mean, CategoricalAccuracy\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4sx1ONGuJwVb"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LACE8w-sJ0sw"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WajPxFupJ7mn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aAJ85kBRx4vk"
      },
      "outputs": [],
      "source": [
        "# gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "# if gpu_devices:\n",
        "#     for gpu in gpu_devices:\n",
        "#         tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsDWR8xp8AOV"
      },
      "source": [
        "### Ensuring Reproducibility of the Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Je9aqmhRzlwz"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.eager import context\n",
        "\n",
        "\n",
        "def set_reprodicibility(seed=42):\n",
        "\n",
        "    # Set random seed for NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Set random seed for TensorFlow\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "    _ = tf.Variable([1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KHrXa-a2FG06"
      },
      "outputs": [],
      "source": [
        "set_reprodicibility()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6eq4JkLFIY8"
      },
      "source": [
        "### Loading CIFAR-10 Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XBCAto1Ie4Fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3d5833-acd4-40c5-b9cf-b33b206d23b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 7s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the CIFAR-10 dataset\n",
        "(train_images, train_labels), (val_images, val_labels) = cifar10.load_data()\n",
        "train_images, val_images = train_images / 255.0, val_images / 255.0\n",
        "train_labels, val_labels = to_categorical(train_labels), to_categorical(val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gt9ALXA5V8Hk"
      },
      "outputs": [],
      "source": [
        "# Convert NumPy arrays to TensorFlow datasets\n",
        "minibatch_size = 256   # 512 in the paper\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(minibatch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).batch(minibatch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNn3bWI8FVeT"
      },
      "source": [
        "### Defining Model Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y80_7NtvYxcR"
      },
      "source": [
        "The suitability of a NN architecture for a particular task, such as CIFAR-10 classification, can depend on various factors including the complexity of the dataset, computational resources available, and the specific requirements of the task.\n",
        "\n",
        "For CIFAR-10, which consists of small (32x32) color images across 10 classes, the dataset is relatively simple compared to larger datasets like ImageNet. Therefore, simpler architectures can often achieve good performance. VGG-style architectures have been widely used and studied for image classification tasks, including CIFAR-10.\n",
        "\n",
        "Researchers often experiment with variations of architectures to find the best-performing model. Some commonly used architectures for CIFAR-10 classification include:\n",
        "\n",
        "- **VGG-like architectures**: Configurations similar to VGG with variations in the number of layers, filter sizes, and depths.\n",
        "- **ResNet**: Residual Networks, which introduce skip connections to mitigate vanishing gradient problems, have been shown effective for CIFAR-10.\n",
        "- **DenseNet**: Dense Convolutional Networks connect each layer to every other layer in a feed-forward fashion, promoting feature reuse and denser gradients.\n",
        "\n",
        "These architectures are often experimented with different hyperparameters such as learning rates, batch sizes, and optimizers to achieve optimal performance. Researchers might also explore techniques like data augmentation, regularization, and learning rate scheduling to improve model performance.\n",
        "\n",
        "Ultimately, the choice of architecture and configuration often involves experimentation to find the best-performing model for a specific dataset and task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSIsCmXZZU37"
      },
      "source": [
        "#### VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5_lD8w1GgT3W"
      },
      "outputs": [],
      "source": [
        "class VGGModel:\n",
        "\n",
        "    def __init__(self, input_shape=(32, 32, 3), num_classes=10):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = models.Sequential()\n",
        "\n",
        "        # Block 1\n",
        "        model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=self.input_shape))\n",
        "        model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "        # Block 2\n",
        "        model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "        # Block 3\n",
        "        model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "        # Block 4\n",
        "        model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "        # Block 5\n",
        "        model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "        # Flatten\n",
        "        model.add(layers.Flatten())\n",
        "\n",
        "        # Dense layers\n",
        "        model.add(layers.Dense(4096, activation='relu'))\n",
        "        model.add(layers.Dense(4096, activation='relu'))\n",
        "        model.add(layers.Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']):\n",
        "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    def summary(self):\n",
        "        self.model.summary()\n",
        "\n",
        "    def save(self, directory, name):\n",
        "        self.model._name = name\n",
        "        self.model.save(directory+name+'.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNPap-2tZYom"
      },
      "source": [
        "#### Simplified VGG Version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvq8U4P7eokb"
      },
      "source": [
        "The following simplified VGG model has three convolutional blocks with decreasing spatial dimensions followed by two fully connected layers. It should be suitable for training on CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3EVIt1AsZXys"
      },
      "outputs": [],
      "source": [
        "class SimpleVGG:\n",
        "\n",
        "    def __init__(self, input_shape=(32, 32, 3), num_classes=10):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        model = Sequential([\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=self.input_shape),\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "            Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "            Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Flatten(),\n",
        "            Dense(512, activation='relu'),\n",
        "            Dense(self.num_classes, activation='softmax')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']):\n",
        "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    def summary(self):\n",
        "        self.model.summary()\n",
        "\n",
        "    def save(self, directory, name):\n",
        "        self.model._name = name\n",
        "        self.model.save(directory+name+'.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PjcvkR8Ydil"
      },
      "source": [
        "#### Simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oekSBXQwYc1f"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN:\n",
        "\n",
        "    def __init__(self, input_shape=(32, 32, 3), num_classes=10):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        model = Sequential([\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=self.input_shape),\n",
        "            Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            MaxPooling2D((2, 2)),\n",
        "\n",
        "            Flatten(),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dense(self.num_classes, activation='softmax')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def compile_model(self, optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']):\n",
        "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "    def summary(self):\n",
        "        self.model.summary()\n",
        "\n",
        "    def save(self, directory, name):\n",
        "        self.model._name = name\n",
        "        self.model.save(directory+name+'.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaBTgHbJzlw1"
      },
      "source": [
        "## TRAINING (Simple CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YKuFdW5P8LVr"
      },
      "outputs": [],
      "source": [
        "# Define the number of runs\n",
        "num_runs = 1\n",
        "epochs = 50\n",
        "\n",
        "tb_log_dir = \"logs/fit/simpleCNN_256/\" #  TensorBoard logs directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f7pzhKWn8LVr"
      },
      "outputs": [],
      "source": [
        "path_to_saved_models = 'output/models/simpleCNN_256/'  # path to store trained models\n",
        "\n",
        "# Check if folder exists\n",
        "if not os.path.exists(path_to_saved_models):\n",
        "    os.makedirs(path_to_saved_models)         # create a new directory of doesn't exist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0OaNiX4FdR7"
      },
      "source": [
        "### SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgGVi-RI8LVr"
      },
      "source": [
        "#### Defining LR schedule, early stopping, tensorboard callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_Xzx5Ja1g-zO"
      },
      "outputs": [],
      "source": [
        "# # Create an instance of the VGGModel class with SGD optimizer\n",
        "# model_sgd = SimpleCNN(input_shape=(32, 32, 3), num_classes=10)\n",
        "# model_sgd.compile_model(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# model_sgd.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcE8Oy8lIqf9"
      },
      "source": [
        "In the SGD-SA paper, they use Scheduled-SGD implementation of SGD. It is quite basic but still rather eﬀective on CIFAR-10 dataset: it uses no momentum/Nesterov acceleration, and the learning rate is set according the following schedule: η =0.1 for ﬁrst 30 epochs, 0.01 for the next 40 epochs, and 0.001 for the ﬁnal 30 epochs.\n",
        "\n",
        "We have modified it for our SimplifiesVGG: learning rate is 0.1 for the ﬁrst 20 epochs, 0.01 for the next 30 epochs, and 0.001 for the ﬁnal epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-llH94DbIWiE"
      },
      "outputs": [],
      "source": [
        "# Define the learning rate schedule function\n",
        "def lr_schedule(epoch):\n",
        "    if epoch <= 10:\n",
        "        return 0.1\n",
        "    elif epoch <= 20:\n",
        "        return 0.01     # lr defaults to 0.01 in SGD\n",
        "    else:\n",
        "        return 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "759IQ09Pd_Du"
      },
      "outputs": [],
      "source": [
        "class EarlyStoppingAt1Accuracy(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('accuracy') >= 0.99:\n",
        "            print(\"\\nReached 100% accuracy so cancelling training!\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "\n",
        "# # Possible Alternative: stop when accuracy is almost not changing:\n",
        "#\n",
        "# early_stopping = EarlyStopping(monitor='accuracy',  # Monitor training accuracy\n",
        "#                                min_delta=0.01,      # Minimum change in accuracy to qualify as an improvement\n",
        "#                                patience=20,         # Number of epochs with no improvement after which training will be stopped\n",
        "#                                verbose=1)           # Print messages about early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iIbgmVGDZO_m"
      },
      "outputs": [],
      "source": [
        "# Define Training Parameters\n",
        "learning_rate_scheduler = LearningRateScheduler(lr_schedule) # Create the LearningRateScheduler callback\n",
        "early_stopping_callback = EarlyStoppingAt1Accuracy() # Define the EarlyStopping callback\n",
        "\n",
        "\n",
        "# # Training\n",
        "# history_sgd = model_sgd.model.fit(train_dataset,\n",
        "#                                   epochs=epochs,\n",
        "#                                   validation_data=val_dataset,\n",
        "#                                   callbacks=[learning_rate_scheduler, early_stopping_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XKQ1uJpXBun4"
      },
      "outputs": [],
      "source": [
        "# # Save the model in native Keras format\n",
        "# model_sgd.save(path_to_saved_models, name='simpleCNN_sgd_'+str(minibatch_size))\n",
        "\n",
        "# # To load the saved model later:\n",
        "# # loaded_model = tf.keras.models.load_model(model_full_path)\n",
        "\n",
        "# # Save the training history\n",
        "# with open(path_to_saved_models+'simpleCNN_sgd_'+str(minibatch_size)+'_history.pkl', 'wb') as file:\n",
        "#     pickle.dump(history_sgd.history, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BaP8xnaXox3l"
      },
      "outputs": [],
      "source": [
        "# Define custom callback to log metrics to the same directory\n",
        "class CustomTensorBoard(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, log_dir):\n",
        "        super(CustomTensorBoard, self).__init__()\n",
        "        self.writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        with self.writer.as_default():\n",
        "            for name, value in logs.items():\n",
        "                tf.summary.scalar(name, value, step=epoch)\n",
        "            self.writer.flush()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnz18GKl8LVs"
      },
      "source": [
        "#### Training Runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "6mUIEz1UiCgQ",
        "outputId": "fbe8fc4b-dffb-4112-b5f6-ebcbb0780c52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "Training run 1/1\n",
            "-----\n",
            "Epoch 1/50\n",
            "162/196 [=======================>......] - ETA: 1:09 - loss: 2.1743 - accuracy: 0.2091"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-eeb3f16e74eb>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Train the model with validation data and log both training and validation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     history_sgd = model_sgd.model.fit(train_dataset,\n\u001b[0m\u001b[1;32m     20\u001b[0m                             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## RUN MULTIPLE TIMES\n",
        "\n",
        "# Loop for each run\n",
        "for run in range(num_runs):\n",
        "\n",
        "    print(\"-----\")\n",
        "    print(f\"Training run {run + 1}/{num_runs}\")\n",
        "    print(\"-----\")\n",
        "\n",
        "    model_sgd = SimpleCNN(input_shape=(32, 32, 3), num_classes=10)\n",
        "    model_sgd.compile_model(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Define TensorBoard log directory for the current run\n",
        "    log_dir = tb_log_dir+\"sgd_run_\" + str(run + 1)\n",
        "    #tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    custom_tensorboard_callback = CustomTensorBoard(log_dir)\n",
        "\n",
        "    # Train the model with validation data and log both training and validation metrics\n",
        "    history_sgd = model_sgd.model.fit(train_dataset,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=val_dataset,\n",
        "                            callbacks=[learning_rate_scheduler, early_stopping_callback, custom_tensorboard_callback])\n",
        "\n",
        "    # SAVES\n",
        "    # Save the trained model in native Keras format\n",
        "    model_sgd.save(path_to_saved_models, name='simpleCNN_sgd_'+str(minibatch_size)+'_run_'+str(run+1))\n",
        "    # Save the training history\n",
        "    with open(path_to_saved_models+'simpleCNN_sgd_'+str(minibatch_size)+'_run_'+str(run+1)+'_history.pkl', 'wb') as file:\n",
        "        pickle.dump(history_sgd.history, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2GZfgWK8LVs"
      },
      "source": [
        "### SGD-SA (attempt to re-produce the known algorithm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NY0iAvAzlw1"
      },
      "source": [
        "#### 1. Defining an Acceptance Criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFb14kLg6Azg"
      },
      "outputs": [],
      "source": [
        "def accept_update(old_loss, new_loss, temperature):\n",
        "\n",
        "    # Metropolis acceptance criteria\n",
        "    worsening = new_loss - old_loss\n",
        "    try:\n",
        "        probability = np.exp((-worsening) / temperature)   # check for an extremely large number (when the temperature gets small)\n",
        "    except RuntimeWarning as e:\n",
        "        print(\"Caught a RuntimeWarning:\", e)\n",
        "        is_worse = worsening > 0\n",
        "        return True, False, 1\n",
        "\n",
        "    #probability = np.exp((-worsening) / temperature)\n",
        "    random_number = random.random()\n",
        "\n",
        "    is_accepted = random_number < probability\n",
        "    is_worse = worsening > 0\n",
        "\n",
        "    return is_accepted, is_worse, probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umusrs0N8LVz"
      },
      "source": [
        "#### 2. Defining a Generator Function to Pick LR based on 'lr_options'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLricPbkV8Hm"
      },
      "outputs": [],
      "source": [
        "# Define a generator function to pick the learning rate based on lr_options\n",
        "def pick_lr(lr_options, seed=123):\n",
        "    #np.random.seed(seed)\n",
        "    while True:\n",
        "\n",
        "        if isinstance(lr_options, list):\n",
        "            yield np.random.choice(lr_options)\n",
        "\n",
        "        elif isinstance(lr_options, dict):\n",
        "            lr_min = lr_options['min_lr']\n",
        "            lr_max = lr_options['max_lr']\n",
        "            yield np.random.uniform(lr_min, lr_max)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"lr_options must be either a list or a dictionary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSNVRBIo8LVz"
      },
      "source": [
        "#### 3. Defining a Customized Training Loop for SGD-SA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQt12V3K8LVz"
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "### Customized Training Loop for SGD-SA ### !!!! MODIFICATION WITH TENSORBOARD\n",
        "###########################################\n",
        "\n",
        "def run_SGD_SA(model, model_name, train_dataset, val_dataset, lr_options,\n",
        "               epochs=100, temperature=1, cooling=0.95, loss_fn=tf.keras.losses.CategoricalCrossentropy(),\n",
        "               output_folder='output/models/'):\n",
        "\n",
        "    # Initialize metrics objects for both training and validation\n",
        "    train_loss_metric = tf.keras.metrics.Mean()  # to average training loss over all batches within the epoch\n",
        "    train_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "    val_loss_metric = tf.keras.metrics.Mean()  # to average training loss over all batches within the epoch\n",
        "    val_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    # Initialize empty lists to store loss and accuracy values (to construct \"history\")\n",
        "    train_loss_history = []\n",
        "    train_accuracy_history = []\n",
        "    val_loss_history = []\n",
        "    val_accuracy_history = []\n",
        "    temperature_history = []\n",
        "    rejected_history = []\n",
        "    accepted_worse_history = []\n",
        "    probability_history = []\n",
        "\n",
        "    # Extract the batch size from the first batch (will be used to name the trained model while saving)\n",
        "    first_batch = next(iter(train_dataset.take(1).as_numpy_iterator()))\n",
        "    batch_size_tensor = tf.shape(first_batch[0])[0]\n",
        "    batch_size_value = batch_size_tensor.numpy()\n",
        "    print(\"batch_size:\", batch_size_value)\n",
        "\n",
        "    # Create the pick_lr generator\n",
        "    lr_generator = pick_lr(lr_options) #, seed=42)\n",
        "\n",
        "    ##### MAIN TRAINING LOOP #####\n",
        "    with summary_writer.as_default():\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "\n",
        "            # Reset metrics for the new epoch\n",
        "            train_loss_metric.reset_states()\n",
        "            train_accuracy_metric.reset_states()\n",
        "            val_loss_metric.reset_states()\n",
        "            val_accuracy_metric.reset_states()\n",
        "\n",
        "            # Reset epoch-level counts\n",
        "            accepted = 0\n",
        "            rejected = 0\n",
        "            accepted_worse = 0\n",
        "            probabilities = []\n",
        "\n",
        "            ##### TRAINING #####\n",
        "            for batch, (x_batch, y_batch) in tqdm(enumerate(train_dataset), desc=\"batches\"):\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    # Forward pass\n",
        "                    predictions = model.model(x_batch)\n",
        "                    # Compute the loss\n",
        "                    loss = loss_fn(y_batch, predictions)\n",
        "\n",
        "                # Save the current weights and loss as \"old\"\n",
        "                old_weights = model.model.trainable_variables\n",
        "                old_loss = loss.numpy()\n",
        "                #print(\"old_loss\", old_loss)\n",
        "\n",
        "                # Compute Gradients\n",
        "                gradients = tape.gradient(loss, model.model.trainable_variables)\n",
        "\n",
        "                # Select a Learning Rate\n",
        "                random_lr = next(lr_generator)\n",
        "\n",
        "                # Update weights using gradient descent\n",
        "                for var, grad in zip(model.model.trainable_variables, gradients):\n",
        "                    var.assign_sub(random_lr * grad)\n",
        "\n",
        "                # Compute the new Loss (after the weights update)\n",
        "                predictions_after_update = model.model(x_batch)\n",
        "                new_loss = loss_fn(y_batch, predictions_after_update)\n",
        "                # print(\"New Loss after Weight Update:\", new_loss.numpy())\n",
        "\n",
        "\n",
        "                # Check acceptance criteria\n",
        "                is_accepted, is_worse, probability = accept_update(old_loss, new_loss, temperature)\n",
        "                probabilities.append(probability)    # save the probability values for the history\n",
        "                if not is_accepted:\n",
        "                    # Reverting update\n",
        "                    model.model.set_weights(old_weights)\n",
        "                    # Update Criterion Rejection Count\n",
        "                    rejected += 1\n",
        "                    # Update Epoch Metrics\n",
        "                    train_loss_metric.update_state(old_loss)\n",
        "                    train_accuracy_metric.update_state(y_batch, predictions)\n",
        "                else:\n",
        "                    # Update Criterion Acceptance Count\n",
        "                    accepted += 1\n",
        "                    # Update Epoch Metrics\n",
        "                    train_loss_metric.update_state(new_loss)\n",
        "                    train_accuracy_metric.update_state(y_batch, predictions_after_update)\n",
        "                    if is_worse:\n",
        "                        accepted_worse +=1\n",
        "                        # print(f\"Epoch: {epoch + 1}, Batch: {batch}, Previous Loss: {loss.numpy()}, New Loss: {new_loss.numpy()}, LR: {random_lr}, Temperature: {temperature}, A/R: {accepted}/{rejected}\")\n",
        "\n",
        "                # # Print progress\n",
        "                # if batch % 500 == 0:\n",
        "                #     print(f\"Epoch: {epoch + 1}, Batch: {batch}, Previous Loss: {loss.numpy()}, New Loss: {new_loss.numpy()}, LR: {random_lr}, Temperature: {temperature}, A/R: {accepted}/{rejected}\")\n",
        "\n",
        "\n",
        "                # If update of the Temperature - every nth batch\n",
        "                # if (batch + 1) % 10 == 0:  # +1 to start from 1\n",
        "                #     temperature = cooling * temperature\n",
        "\n",
        "            ##### VALIDATION #####\n",
        "            for batch, (x_val_batch, y_val_batch) in enumerate(val_dataset):\n",
        "                # Forward pass\n",
        "                val_predictions = model.model(x_val_batch)\n",
        "                # Compute the validation loss\n",
        "                val_loss = loss_fn(y_val_batch, val_predictions)\n",
        "                # Update validation metrics\n",
        "                val_loss_metric.update_state(val_loss)\n",
        "                val_accuracy_metric.update_state(y_val_batch, val_predictions)\n",
        "\n",
        "\n",
        "            ##### EPOCH RESULTS #####\n",
        "            epoch_loss = train_loss_metric.result().numpy()\n",
        "            epoch_accuracy = train_accuracy_metric.result().numpy()\n",
        "            epoch_val_loss = val_loss_metric.result().numpy()\n",
        "            epoch_val_accuracy = val_accuracy_metric.result().numpy()\n",
        "\n",
        "\n",
        "            # Store the epoch-level metrics to history lists\n",
        "            train_loss_history.append(epoch_loss)\n",
        "            train_accuracy_history.append(epoch_accuracy)\n",
        "            val_loss_history.append(epoch_val_loss)\n",
        "            val_accuracy_history.append(epoch_val_accuracy)\n",
        "            temperature_history.append(temperature)\n",
        "            rejected_history.append(rejected)\n",
        "            accepted_worse_history.append(accepted_worse)\n",
        "            probability_history.append(probabilities)\n",
        "\n",
        "            # Write metrics to the TensorBoard log\n",
        "            tf.summary.scalar('loss', epoch_loss, step=epoch)\n",
        "            tf.summary.scalar('accuracy', epoch_accuracy, step=epoch)\n",
        "            tf.summary.scalar('val_loss', epoch_val_loss, step=epoch)\n",
        "            tf.summary.scalar('val_accuracy', epoch_val_accuracy, step=epoch)\n",
        "            summary_writer.flush() # Flush the summary writer\n",
        "\n",
        "            # Print epoch results\n",
        "            print(f\"Epoch {epoch}/{epochs}\")\n",
        "            print(f\"loss: {epoch_loss:.4f} - accuracy: {epoch_accuracy:.4f} - val_loss: {epoch_val_loss:.4f} - val_accuracy: {epoch_val_accuracy:.4f} - temperature: {temperature:.4f} - rejected: {rejected}/{accepted+rejected} - accepted_worse: {accepted_worse}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if epoch > 2:\n",
        "                recent_accuracy = sum(train_accuracy_history[-2:]) / 2  # Average accuracy over the last 2 epochs\n",
        "                if recent_accuracy >= 0.99:  # Stop training if training accuracy exceeds 0.99\n",
        "                    print(\"Training accuracy reached 0.99. Stopping training.\")\n",
        "                    break\n",
        "\n",
        "\n",
        "            # Update Temperature in the end of the Epoch\n",
        "            temperature = cooling * temperature\n",
        "\n",
        "    ##### Construct the history object #####\n",
        "    history = {\n",
        "        'loss': train_loss_history,\n",
        "        'accuracy': train_accuracy_history,\n",
        "        'val_loss': val_loss_history,\n",
        "        'val_accuracy': val_accuracy_history,\n",
        "        'temperature': temperature_history,\n",
        "        'rejected': rejected_history,\n",
        "        'accepted_worse': accepted_worse_history,\n",
        "        'probabilities': probability_history\n",
        "    }\n",
        "\n",
        "    ##### SAVES #####\n",
        "\n",
        "    # Save the trained model in native Keras format\n",
        "    model.save(output_folder, model_name+'_'+str(batch_size_value)+'.keras')\n",
        "    # To load the saved model later:\n",
        "    # loaded_model = tf.keras.models.load_model(model_save_path)\n",
        "\n",
        "    # Save the training history\n",
        "    with open(output_folder+model_name+'_'+str(batch_size_value)+'_history.pkl', 'wb') as file:\n",
        "        pickle.dump(history, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR2h7QFvhPto"
      },
      "source": [
        "**Important remark for the custom training defined above:**\n",
        "\n",
        "For tracking loss values during training, we use `tf.keras.metrics.Mean()`. This metric calculates the mean of the values observed across all batches.\n",
        "\n",
        "`tf.keras.metrics.CategoricalCrossentropy()` is not suitable for tracking loss values during training. Instead, it's used as a loss function to compute the categorical cross-entropy loss between the true labels and the predicted probabilities.\n",
        "\n",
        "In summary:\n",
        "* Use `tf.keras.metrics.Mean()` to track the loss values during training.\n",
        "* Use `tf.keras.losses.CategoricalCrossentropy()` as the loss function during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l90RN34i8LVz"
      },
      "source": [
        "#### Training Runs for SGD-SA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjHDoAaozlw2"
      },
      "outputs": [],
      "source": [
        "# Define Training Parameters FOR BOTH: SGD-SA and SGD-SA modified\n",
        "# epochs = 30\n",
        "\n",
        "temperature = 1\n",
        "cooling = 0.95  # in the paper: 0.8\n",
        "\n",
        "lr_options =  [0.1, 0.07, 0.05, 0.03, 0.01, 0.007, 0.005, 0.003, 0.001]  # lr defaults to 0.01 in SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9Be5LR-8LVz"
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "\n",
        "# run_SGD_SA(model_sgd_sa, 'simpleCNN_SGD_SA', train_dataset, val_dataset, lr_options,\n",
        "#           epochs, temperature, cooling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1Tdr4TZPRhm"
      },
      "outputs": [],
      "source": [
        "### TESTING MULTIPLE RUNS\n",
        "\n",
        "# Loop for each run\n",
        "for run in range(num_runs):\n",
        "\n",
        "    print(\"-----\")\n",
        "    print(f\"Training run {run + 1}/{num_runs}\")\n",
        "    print(\"-----\")\n",
        "\n",
        "    # Compile the model\n",
        "    model_sgd_sa = SimpleCNN(input_shape=(32, 32, 3), num_classes=10)\n",
        "\n",
        "\n",
        "    # Create a summary writer for each run\n",
        "    summary_writer = tf.summary.create_file_writer(tb_log_dir + \"sgd_sa_\" + f'run_{run + 1}')\n",
        "\n",
        "    # Run training\n",
        "    run_SGD_SA(model_sgd_sa, 'simpleCNN_SGD_SA_'+str(run+1), train_dataset, val_dataset,\n",
        "               lr_options, epochs, temperature, cooling)\n",
        "\n",
        "    print(\"==============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3tAj2eY8LVz"
      },
      "source": [
        "### SGD-SA with Modified LR Picking (random from uniform distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining Parameters for SGD-SA modified"
      ],
      "metadata": {
        "id": "wA59TEI5Jshv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QM6iUli8LVz"
      },
      "outputs": [],
      "source": [
        "# temperature = 1\n",
        "# cooling = 0.95     # in the paper: 0.8\n",
        "\n",
        "\n",
        "lr_options = {\n",
        "    'min_lr': np.min(lr_options),\n",
        "    'max_lr': np.max(lr_options)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Runs for SGD-SA modified"
      ],
      "metadata": {
        "id": "eOlAz4f8JiAG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAxXfCl98LV0"
      },
      "outputs": [],
      "source": [
        "### TESTING MULTIPLE RUNS\n",
        "\n",
        "# Loop for each run\n",
        "for run in range(num_runs):\n",
        "\n",
        "    print(\"-----\")\n",
        "    print(f\"Training run {run + 1}/{num_runs}\")\n",
        "    print(\"-----\")\n",
        "\n",
        "    # Compile the model\n",
        "    model_modified = SimpleCNN(input_shape=(32, 32, 3), num_classes=10)\n",
        "\n",
        "\n",
        "    # Create a summary writer for each run\n",
        "    summary_writer = tf.summary.create_file_writer(tb_log_dir + \"sgd_sa_modified_\" + f'run_{run + 1}')\n",
        "\n",
        "    # Run training\n",
        "    run_SGD_SA(model_modified, 'simpleCNN_SGD_SA_modified'+str(run+1), train_dataset, val_dataset,\n",
        "               lr_options, epochs, temperature, cooling)\n",
        "\n",
        "    print(\"==============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-5-8FtY8LV0"
      },
      "source": [
        "### SGD-SA with Modified LR Picking + Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining function for SGD-SA with Modified LR Picking + Momentum"
      ],
      "metadata": {
        "id": "PVBjm-jtJzNu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU6ZOpRD0AEH"
      },
      "outputs": [],
      "source": [
        "###########################################\n",
        "### Customized Training Loop for SGD-SA ###\n",
        "###########################################\n",
        "def run_SGD_SA_WITH_MOMENTUM(model, model_name, train_dataset, val_dataset, lr_options,\n",
        "               epochs=100, temperature=1, cooling=0.95, loss_fn=tf.keras.losses.CategoricalCrossentropy(),\n",
        "               output_folder='output/models/'):\n",
        "\n",
        "    # Initialize metrics objects for both training and validation\n",
        "    train_loss_metric = tf.keras.metrics.Mean()  # to average training loss over all batches within the epoch\n",
        "    train_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "    val_loss_metric = tf.keras.metrics.Mean()  # to average training loss over all batches within the epoch\n",
        "    val_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    # Initialize empty lists to store loss and accuracy values (to construct \"history\")\n",
        "    train_loss_history = []\n",
        "    train_accuracy_history = []\n",
        "    val_loss_history = []\n",
        "    val_accuracy_history = []\n",
        "    temperature_history = []\n",
        "    rejected_history = []\n",
        "    accepted_worse_history = []\n",
        "    probability_history = []\n",
        "\n",
        "    # Extract the batch size from the first batch (will be used to name the trained model while saving)\n",
        "    first_batch = next(iter(train_dataset.take(1).as_numpy_iterator()))\n",
        "    batch_size_tensor = tf.shape(first_batch[0])[0]\n",
        "    batch_size_value = batch_size_tensor.numpy()\n",
        "    print(\"batch_size:\", batch_size_value)\n",
        "\n",
        "    # Create the pick_lr generator\n",
        "    lr_generator = pick_lr(lr_options) #, seed=42)\n",
        "\n",
        "    # NEW\n",
        "    velocities = [tf.Variable(tf.zeros_like(var), trainable=False) for var in model.model.trainable_variables]\n",
        "    beta = 0.95  # Momentum parameter\n",
        "\n",
        "\n",
        "    ##### MAIN TRAINING LOOP #####\n",
        "    with summary_writer.as_default():\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "\n",
        "            # Reset metrics for the new epoch\n",
        "            train_loss_metric.reset_states()\n",
        "            train_accuracy_metric.reset_states()\n",
        "            val_loss_metric.reset_states()\n",
        "            val_accuracy_metric.reset_states()\n",
        "\n",
        "            # Reset epoch-level counts\n",
        "            accepted = 0\n",
        "            rejected = 0\n",
        "            accepted_worse = 0\n",
        "            probabilities = []\n",
        "\n",
        "            ##### TRAINING #####\n",
        "            for batch, (x_batch, y_batch) in tqdm(enumerate(train_dataset), desc=\"batches\"):\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    # Forward pass\n",
        "                    predictions = model.model(x_batch)\n",
        "                    # Compute the loss\n",
        "                    loss = loss_fn(y_batch, predictions)\n",
        "\n",
        "                # Save the current weights and loss as \"old\"\n",
        "                old_weights = model.model.trainable_variables\n",
        "                old_loss = loss.numpy()\n",
        "                #print(\"old_loss\", old_loss)\n",
        "                # NEW\n",
        "                old_velocities = [tf.identity(vel) for vel in velocities]  # Store velocities for possible reversion\n",
        "\n",
        "\n",
        "                # Compute Gradients\n",
        "                gradients = tape.gradient(loss, model.model.trainable_variables)\n",
        "\n",
        "                # Select a Learning Rate\n",
        "                random_lr = next(lr_generator)\n",
        "\n",
        "                # NEW Update velocities using momentum\n",
        "                for vel, grad in zip(velocities, gradients):\n",
        "                    vel.assign(beta * vel + (1 - beta) * grad)\n",
        "\n",
        "                # Update weights using gradient descent\n",
        "                # for var, grad in zip(model.model.trainable_variables, gradients):\n",
        "                #     var.assign_sub(random_lr * grad)\n",
        "\n",
        "                # Update weights using velocities\n",
        "                for var, vel in zip(model.model.trainable_variables, velocities):\n",
        "                    var.assign_sub(random_lr * vel)\n",
        "\n",
        "\n",
        "                # Compute the new Loss (after the weights update)\n",
        "                predictions_after_update = model.model(x_batch)\n",
        "                new_loss = loss_fn(y_batch, predictions_after_update)\n",
        "                # print(\"New Loss after Weight Update:\", new_loss.numpy())\n",
        "\n",
        "\n",
        "                # Check acceptance criteria\n",
        "                is_accepted, is_worse, probability = accept_update(old_loss, new_loss, temperature)\n",
        "                probabilities.append(probability)    # save the probability values for the history\n",
        "                if not is_accepted:\n",
        "                # Reverting weights and velocities\n",
        "                    model.model.set_weights(old_weights)\n",
        "                    for vel, old_vel in zip(velocities, old_velocities):  #NEW - need to revert velocities as well\n",
        "                        vel.assign(old_vel)\n",
        "                    # Update Criterion Rejection Count\n",
        "                    rejected += 1\n",
        "                    # Update Epoch Metrics\n",
        "                    train_loss_metric.update_state(old_loss)\n",
        "                    train_accuracy_metric.update_state(y_batch, predictions)\n",
        "                else:\n",
        "                    # Update Criterion Acceptance Count\n",
        "                    accepted += 1\n",
        "                    # Update Epoch Metrics\n",
        "                    train_loss_metric.update_state(new_loss)\n",
        "                    train_accuracy_metric.update_state(y_batch, predictions_after_update)\n",
        "                    if is_worse:\n",
        "                        accepted_worse +=1\n",
        "                        # print(f\"Epoch: {epoch + 1}, Batch: {batch}, Previous Loss: {loss.numpy()}, New Loss: {new_loss.numpy()}, LR: {random_lr}, Temperature: {temperature}, A/R: {accepted}/{rejected}\")\n",
        "\n",
        "                # # Print progress\n",
        "                # if batch % 500 == 0:\n",
        "                #     print(f\"Epoch: {epoch + 1}, Batch: {batch}, Previous Loss: {loss.numpy()}, New Loss: {new_loss.numpy()}, LR: {random_lr}, Temperature: {temperature}, A/R: {accepted}/{rejected}\")\n",
        "\n",
        "\n",
        "                # If update of the Temperature - every nth batch\n",
        "                # if (batch + 1) % 10 == 0:  # +1 to start from 1\n",
        "                #     temperature = cooling * temperature\n",
        "\n",
        "            ##### VALIDATION #####\n",
        "            for batch, (x_val_batch, y_val_batch) in enumerate(val_dataset):\n",
        "                # Forward pass\n",
        "                val_predictions = model.model(x_val_batch)\n",
        "                # Compute the validation loss\n",
        "                val_loss = loss_fn(y_val_batch, val_predictions)\n",
        "                # Update validation metrics\n",
        "                val_loss_metric.update_state(val_loss)\n",
        "                val_accuracy_metric.update_state(y_val_batch, val_predictions)\n",
        "\n",
        "\n",
        "\n",
        "            ##### EPOCH RESULTS #####\n",
        "            epoch_loss = train_loss_metric.result().numpy()\n",
        "            epoch_accuracy = train_accuracy_metric.result().numpy()\n",
        "            epoch_val_loss = val_loss_metric.result().numpy()\n",
        "            epoch_val_accuracy = val_accuracy_metric.result().numpy()\n",
        "\n",
        "\n",
        "            # Store the epoch-level metrics to history lists\n",
        "            train_loss_history.append(epoch_loss)\n",
        "            train_accuracy_history.append(epoch_accuracy)\n",
        "            val_loss_history.append(epoch_val_loss)\n",
        "            val_accuracy_history.append(epoch_val_accuracy)\n",
        "            temperature_history.append(temperature)\n",
        "            rejected_history.append(rejected)\n",
        "            accepted_worse_history.append(accepted_worse)\n",
        "            probability_history.append(probabilities)\n",
        "\n",
        "            # Write metrics to the TensorBoard log\n",
        "            tf.summary.scalar('loss', epoch_loss, step=epoch)\n",
        "            tf.summary.scalar('accuracy', epoch_accuracy, step=epoch)\n",
        "            tf.summary.scalar('val_loss', epoch_val_loss, step=epoch)\n",
        "            tf.summary.scalar('val_accuracy', epoch_val_accuracy, step=epoch)\n",
        "            summary_writer.flush() # Flush the summary writer\n",
        "\n",
        "            # Print epoch results\n",
        "            print(f\"Epoch {epoch}/{epochs}\")\n",
        "            print(f\"loss: {epoch_loss:.4f} - accuracy: {epoch_accuracy:.4f} - val_loss: {epoch_val_loss:.4f} - val_accuracy: {epoch_val_accuracy:.4f} - temperature: {temperature:.4f} - rejected: {rejected}/{accepted+rejected} - accepted_worse: {accepted_worse}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if epoch > 2:\n",
        "                recent_accuracy = sum(train_accuracy_history[-2:]) / 2  # Average accuracy over the last 2 epochs\n",
        "                if recent_accuracy >= 0.99:  # Stop training if training accuracy exceeds 0.99\n",
        "                    print(\"Training accuracy reached 0.99. Stopping training.\")\n",
        "                    break\n",
        "\n",
        "\n",
        "            # Update Temperature in the end of the Epoch\n",
        "            temperature = cooling * temperature\n",
        "\n",
        "\n",
        "    ##### Construct the history object #####\n",
        "    history = {\n",
        "        'loss': train_loss_history,\n",
        "        'accuracy': train_accuracy_history,\n",
        "        'val_loss': val_loss_history,\n",
        "        'val_accuracy': val_accuracy_history,\n",
        "        'temperature': temperature_history,\n",
        "        'rejected': rejected_history,\n",
        "        'accepted_worse': accepted_worse_history,\n",
        "        'probabilities': probability_history\n",
        "    }\n",
        "\n",
        "    ##### SAVES #####\n",
        "\n",
        "    # Save the trained model in native Keras format\n",
        "    model.save(output_folder, model_name+'_'+str(batch_size_value)+'.keras')\n",
        "    # To load the saved model later:\n",
        "    # loaded_model = tf.keras.models.load_model(model_save_path)\n",
        "\n",
        "    # Save the training history\n",
        "    with open(output_folder+model_name+'_'+str(batch_size_value)+'_history.pkl', 'wb') as file:\n",
        "        pickle.dump(history, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting Parameters for SGD-SA with Modified LR Picking + Momentum"
      ],
      "metadata": {
        "id": "E3ZFHWUpJ94a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luZQLPRp8LV0"
      },
      "outputs": [],
      "source": [
        "# temperature = 1\n",
        "# cooling = 0.8     # in the paper: 0.8\n",
        "\n",
        "# lr_options = {\n",
        "#     'min_lr': np.min(lr_options),\n",
        "#     'max_lr': np.max(lr_options)\n",
        "# }\n",
        "\n",
        "lr_options"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Runs for SGD-SA with Modified LR Picking + Momentum"
      ],
      "metadata": {
        "id": "7Di6RTWoKEkZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-R4Sefc8LV0"
      },
      "outputs": [],
      "source": [
        "### TESTING MULTIPLE RUNS\n",
        "\n",
        "# Loop for each run\n",
        "for run in range(num_runs):\n",
        "\n",
        "    print(\"-----\")\n",
        "    print(f\"Training run {run + 1}/{num_runs}\")\n",
        "    print(\"-----\")\n",
        "\n",
        "    # Compile the model\n",
        "    model = SimpleCNN(input_shape=(32, 32, 3), num_classes=10)\n",
        "\n",
        "\n",
        "    # Create a summary writer for each run\n",
        "    summary_writer = tf.summary.create_file_writer(tb_log_dir + \"sgd_sa_modified_with_momentum_\" + f'run_{run + 1}')\n",
        "\n",
        "    # Run training of SGD-SA with momentum\n",
        "    run_SGD_SA_WITH_MOMENTUM(model, 'simpleCNN_SGD_SA_modified_momentum_'+str(run+1), train_dataset, val_dataset,\n",
        "               lr_options, epochs, temperature, cooling)\n",
        "\n",
        "    print(\"==============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHg6JG38LV0"
      },
      "source": [
        "### SGD-SA utilizing Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining Accuracy and Acceptance Criteria based on Accuracy"
      ],
      "metadata": {
        "id": "sFVnLYOcKLzW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHaIFe8y8LV0"
      },
      "outputs": [],
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "    # Convert predicted probabilities to predicted class labels\n",
        "    y_pred_labels = tf.argmax(y_pred, axis=1)\n",
        "    # Convert true labels to integer format (assuming one-hot encoded)\n",
        "    y_true_labels = tf.argmax(y_true, axis=1)\n",
        "    # Compare predicted labels with true labels\n",
        "    correct_predictions = tf.equal(y_true_labels, y_pred_labels)\n",
        "    # Compute accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accept_update__acc(old_accuracy, new_accuracy, temperature):\n",
        "\n",
        "    # Metropolis acceptance criteria\n",
        "    worsening = old_accuracy - new_accuracy # if >0 then the neigbour solution is worse\n",
        "    try:\n",
        "        probability = np.exp((-worsening) / temperature)   # check for an extremely large number (when the temperature gets small)\n",
        "    except RuntimeWarning as e:\n",
        "        print(\"Caught a RuntimeWarning:\", e)\n",
        "        is_worse = worsening > 0\n",
        "        return True, False, 1\n",
        "\n",
        "    #probability = np.exp((-worsening) / temperature)\n",
        "    random_number = random.random()\n",
        "\n",
        "    is_accepted = random_number < probability\n",
        "    is_worse = worsening > 0\n",
        "\n",
        "    return is_accepted, is_worse, probability"
      ],
      "metadata": {
        "id": "rxTKFFOf_C0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining function for SGD-SA utilizing accuracy"
      ],
      "metadata": {
        "id": "uFuv8PlKKStd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fCKfVrx8LV0"
      },
      "outputs": [],
      "source": [
        "def run_SGD_SA__acc(model, model_name, train_dataset, val_dataset, lr_options,\n",
        "               epochs=100, temperature=1, cooling=0.95, loss_fn=tf.keras.losses.CategoricalCrossentropy(),\n",
        "               output_folder='output/models/'):\n",
        "\n",
        "    # Initialize metrics objects for both training and validation\n",
        "    train_loss_metric = tf.keras.metrics.Mean()  # to average training loss over all batches within the epoch\n",
        "    train_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "    val_loss_metric = tf.keras.metrics.Mean()  # to average training loss over all batches within the epoch\n",
        "    val_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "    # Initialize empty lists to store loss and accuracy values (to construct \"history\")\n",
        "    train_loss_history = []\n",
        "    train_accuracy_history = []\n",
        "    val_loss_history = []\n",
        "    val_accuracy_history = []\n",
        "    temperature_history = []\n",
        "    rejected_history = []\n",
        "    accepted_worse_history = []\n",
        "    probability_history = []\n",
        "\n",
        "    # Extract the batch size from the first batch (will be used to name the trained model while saving)\n",
        "    first_batch = next(iter(train_dataset.take(1).as_numpy_iterator()))\n",
        "    batch_size_tensor = tf.shape(first_batch[0])[0]\n",
        "    batch_size_value = batch_size_tensor.numpy()\n",
        "    print(\"batch_size:\", batch_size_value)\n",
        "\n",
        "    # Create the pick_lr generator\n",
        "    lr_generator = pick_lr(lr_options) #, seed=42)\n",
        "\n",
        "    ##### MAIN TRAINING LOOP #####\n",
        "    with summary_writer.as_default():\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "\n",
        "            # Reset metrics for the new epoch\n",
        "            train_loss_metric.reset_states()\n",
        "            train_accuracy_metric.reset_states()\n",
        "            val_loss_metric.reset_states()\n",
        "            val_accuracy_metric.reset_states()\n",
        "\n",
        "            # Reset epoch-level counts\n",
        "            accepted = 0\n",
        "            rejected = 0\n",
        "            accepted_worse = 0\n",
        "            probabilities = []\n",
        "\n",
        "            ##### TRAINING #####\n",
        "            for batch, (x_batch, y_batch) in tqdm(enumerate(train_dataset), desc=\"batches\"):\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    # Forward pass\n",
        "                    predictions = model.model(x_batch)\n",
        "                    # Compute the loss\n",
        "                    loss = loss_fn(y_batch, predictions)\n",
        "                    # Compute the accuracy\n",
        "                    accuracy = accuracy_fn(y_batch, predictions)\n",
        "\n",
        "                # Save the current weights and loss as \"old\"\n",
        "                old_weights = model.model.trainable_variables\n",
        "                old_loss = loss.numpy()\n",
        "                old_accuracy = accuracy.numpy()\n",
        "                #print(\"old_loss\", old_loss)\n",
        "\n",
        "                # Compute Gradients\n",
        "                gradients = tape.gradient(loss, model.model.trainable_variables)\n",
        "\n",
        "                # Select a Learning Rate\n",
        "                random_lr = next(lr_generator)\n",
        "\n",
        "                # Update weights using gradient descent\n",
        "                for var, grad in zip(model.model.trainable_variables, gradients):\n",
        "                    var.assign_sub(random_lr * grad)\n",
        "\n",
        "                # Compute the new Loss (after the weights update)\n",
        "                predictions_after_update = model.model(x_batch)\n",
        "                new_loss = loss_fn(y_batch, predictions_after_update)\n",
        "                new_accuracy = accuracy_fn(y_batch, predictions_after_update)\n",
        "                # print(\"New Loss after Weight Update:\", new_loss.numpy())\n",
        "\n",
        "                # Check acceptance criteria !!! BASED ON ACCURACY\n",
        "                #is_accepted, is_worse, probability = accept_update(old_loss, new_loss, temperature)\n",
        "                is_accepted, is_worse, probability = accept_update__acc(old_accuracy, new_accuracy, temperature)\n",
        "                probabilities.append(probability)    # save the probability values for the history\n",
        "\n",
        "                if not is_accepted:\n",
        "                    # Reverting update\n",
        "                    model.model.set_weights(old_weights)\n",
        "                    # Update Criterion Rejection Count\n",
        "                    rejected += 1\n",
        "                    # Update Epoch Metrics\n",
        "                    train_loss_metric.update_state(old_loss)\n",
        "                    train_accuracy_metric.update_state(y_batch, predictions)\n",
        "                else:\n",
        "                    # Update Criterion Acceptance Count\n",
        "                    accepted += 1\n",
        "                    # Update Epoch Metrics\n",
        "                    train_loss_metric.update_state(new_loss)\n",
        "                    train_accuracy_metric.update_state(y_batch, predictions_after_update)\n",
        "                    if is_worse:\n",
        "                        accepted_worse +=1\n",
        "                        # print(f\"Epoch: {epoch + 1}, Batch: {batch}, Previous Loss: {loss.numpy()}, New Loss: {new_loss.numpy()}, LR: {random_lr}, Temperature: {temperature}, A/R: {accepted}/{rejected}\")\n",
        "\n",
        "                # # Print progress\n",
        "                # if batch % 500 == 0:\n",
        "                #     print(f\"Epoch: {epoch + 1}, Batch: {batch}, Previous Loss: {loss.numpy()}, New Loss: {new_loss.numpy()}, LR: {random_lr}, Temperature: {temperature}, A/R: {accepted}/{rejected}\")\n",
        "\n",
        "\n",
        "                # If update of the Temperature - every nth batch\n",
        "                # if (batch + 1) % 10 == 0:  # +1 to start from 1\n",
        "                #     temperature = cooling * temperature\n",
        "\n",
        "\n",
        "            ##### VALIDATION #####\n",
        "            for batch, (x_val_batch, y_val_batch) in enumerate(val_dataset):\n",
        "                # Forward pass\n",
        "                val_predictions = model.model(x_val_batch)\n",
        "                # Compute the validation loss\n",
        "                val_loss = loss_fn(y_val_batch, val_predictions)\n",
        "                # Update validation metrics\n",
        "                val_loss_metric.update_state(val_loss)\n",
        "                val_accuracy_metric.update_state(y_val_batch, val_predictions)\n",
        "\n",
        "\n",
        "            ##### EPOCH RESULTS #####\n",
        "\n",
        "            epoch_loss = train_loss_metric.result().numpy()\n",
        "            epoch_accuracy = train_accuracy_metric.result().numpy()\n",
        "            epoch_val_loss = val_loss_metric.result().numpy()\n",
        "            epoch_val_accuracy = val_accuracy_metric.result().numpy()\n",
        "\n",
        "\n",
        "            # Store the epoch-level metrics to history lists\n",
        "            train_loss_history.append(epoch_loss)\n",
        "            train_accuracy_history.append(epoch_accuracy)\n",
        "            val_loss_history.append(epoch_val_loss)\n",
        "            val_accuracy_history.append(epoch_val_accuracy)\n",
        "            temperature_history.append(temperature)\n",
        "            rejected_history.append(rejected)\n",
        "            accepted_worse_history.append(accepted_worse)\n",
        "            probability_history.append(probabilities)\n",
        "\n",
        "            # Write metrics to the TensorBoard log\n",
        "            tf.summary.scalar('loss', epoch_loss, step=epoch)\n",
        "            tf.summary.scalar('accuracy', epoch_accuracy, step=epoch)\n",
        "            tf.summary.scalar('val_loss', epoch_val_loss, step=epoch)\n",
        "            tf.summary.scalar('val_accuracy', epoch_val_accuracy, step=epoch)\n",
        "            summary_writer.flush() # Flush the summary writer\n",
        "\n",
        "            # Print epoch results\n",
        "            print(f\"Epoch {epoch}/{epochs}\")\n",
        "            print(f\"loss: {epoch_loss:.4f} - accuracy: {epoch_accuracy:.4f} - val_loss: {epoch_val_loss:.4f} - val_accuracy: {epoch_val_accuracy:.4f} - temperature: {temperature:.4f} - rejected: {rejected}/{accepted+rejected} - accepted_worse: {accepted_worse}\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if epoch > 2:\n",
        "                recent_accuracy = sum(train_accuracy_history[-2:]) / 2  # Average accuracy over the last 2 epochs\n",
        "                if recent_accuracy >= 0.99:  # Stop training if training accuracy exceeds 0.99\n",
        "                    print(\"Training accuracy reached 0.99. Stopping training.\")\n",
        "                    break\n",
        "\n",
        "\n",
        "            # Update Temperature in the end of the Epoch\n",
        "            temperature = cooling * temperature\n",
        "\n",
        "\n",
        "\n",
        "    ##### Construct the history object #####\n",
        "    history = {\n",
        "        'loss': train_loss_history,\n",
        "        'accuracy': train_accuracy_history,\n",
        "        'val_loss': val_loss_history,\n",
        "        'val_accuracy': val_accuracy_history,\n",
        "        'temperature': temperature_history,\n",
        "        'rejected': rejected_history,\n",
        "        'accepted_worse': accepted_worse_history,\n",
        "        'probabilities': probability_history\n",
        "    }\n",
        "\n",
        "    ##### SAVES #####\n",
        "\n",
        "    # Save the trained model in native Keras format\n",
        "    model.save(output_folder, model_name+'_'+str(batch_size_value)+'.keras')\n",
        "    # To load the saved model later:\n",
        "    # loaded_model = tf.keras.models.load_model(model_save_path)\n",
        "\n",
        "    # Save the training history\n",
        "    with open(output_folder+model_name+'_'+str(batch_size_value)+'_history.pkl', 'wb') as file:\n",
        "        pickle.dump(history, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Runs for SGD-SA Utilizing Accuracy"
      ],
      "metadata": {
        "id": "ngjd9TsPKZMj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYUH62Qo8LV0"
      },
      "outputs": [],
      "source": [
        "### TESTING MULTIPLE RUNS\n",
        "\n",
        "# Loop for each run\n",
        "for run in range(num_runs):\n",
        "\n",
        "    print(\"-----\")\n",
        "    print(f\"Training run {run + 1}/{num_runs}\")\n",
        "    print(\"-----\")\n",
        "\n",
        "    # Compile the model\n",
        "    model = SimpleCNN(input_shape=(32, 32, 3), num_classes=10)\n",
        "\n",
        "\n",
        "    # Create a summary writer for each run\n",
        "    summary_writer = tf.summary.create_file_writer(tb_log_dir + \"sgd_sa_modified__acc_\" + f'run_{run + 1}')\n",
        "\n",
        "    # Run training of SGD-SA with momentum\n",
        "    run_SGD_SA__acc(model, 'simpleCNN_SGD_SA_modified__acc_'+str(run+1), train_dataset, val_dataset,\n",
        "               lr_options, epochs, temperature, cooling)\n",
        "\n",
        "    print(\"==============\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-tFhgxV5c0h"
      },
      "source": [
        "## Visualizing Results with TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu54FlU98LV1"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit/simpleCNN_256/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zqQgX258LV1"
      },
      "outputs": [],
      "source": [
        "# Open TensorBoard in web browser:\n",
        "# http://localhost:6006/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zip folders containing output artifacts"
      ],
      "metadata": {
        "id": "jn7kSc5IGik9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItMfB_6B8LV1"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Function to zip a directory\n",
        "def zip_dir(folder_path, zip_name):\n",
        "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(folder_path, '..')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXwtxooF8LV1"
      },
      "outputs": [],
      "source": [
        "# Create zip files of the desired directories\n",
        "zip_dir(\"logs/\", 'logs.zip')\n",
        "zip_dir(\"output/\", 'output.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX3OhDB18LV1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJOupiTZk31y"
      },
      "outputs": [],
      "source": [
        "### TO DO:\n",
        "\n",
        "\n",
        "## FIXES:\n",
        "\n",
        "# 0. !!!Re-train and Visualize results on graphs\n",
        "\n",
        "# 1. Check Reproducibility! (current seeds are not enough!)\n",
        "\n",
        "# 2. Fix Temperature values after cooling - remove insignificant digits\n",
        "\n",
        "# 3. Think about \"RuntimeWarning: overflow encountered in exp probability = np.exp((-worsening) / temperature)\"\n",
        "\n",
        "\n",
        "\n",
        "## METHODOLOGY IDEAS:\n",
        "\n",
        "# 1. Try modifying the acceptance criterion, for example, to make it similar to fast SA\n",
        "\n",
        "# 2. Try calculating 2 neighbors at a time and taking the best one (analyze results afterward)\n",
        "\n",
        "# 3. Modification of LR picking: Try generating LR as normally distributed around default 0.01\n",
        "\n",
        "# 4. Try using 2 two diﬀerent objective functions at training time:\n",
        "# one diﬀerentiable to compute the gradient (and hence a set of potentially good moves),\n",
        "# and another completely generic (possibly black box) for the SA acceptance/rejection test\n",
        "# —the latter intended to favor simple/robust solutions that are likely to generalize well.\n",
        "\n",
        "# 6. Whatever we end up with, run multiple times for the \"statistical significance\" of the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8iu2fO58LV1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".tfenv",
      "language": "python",
      "name": ".tfenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}